20190422  
国立情報学研究所

# SQuAD
- 計算機の「読解力」を数値化  
https://rajpurkar.github.io/SQuAD-explorer/  
https://qiita.com/MeguruMokke/items/44720220d4fe9de3f804
- テキスト・質問文・正解のデータセット
    - 質問文の文字列がテキストの文字鉄に重なる場合もある→脳死で解ける
    - 質問文のフレーズだけテキストから探してそのあたりから解答すると間違うような設計の問題もある
    - 文をまたぐ推論，代名詞の参照といった問題もある
    - 人間の正答率は86.8％に対してLeaderboard1位は87.1％なので，問題設計自体を疑問視する人もいるらしい
- みんなDLでやってるらしい
    - 答えに使えそうな場所を重み付け

# 言語の統計的性質
- Zipfの法則
    - 単語の出現頻度は順位に反比例(log(y) = -k * log(x)，両対数スケールで直線)
- Pareto分布（べき分布のひとつ）
    - 生成モデル
        1. 語は，すでに観察された語の頻度に比例する確率で生成される
        2. 一定間隔で，これまで観察されていない語が生成される
    - 未知語生成があるので，語の出現確率を単純に 出現回数 / 総語数 にしてはいけない→未知語のぶんの補正(smoothing, discounting)
    - シャノンゲーム： つぎに来る語の予想
        - n-gram: n語の組み合わせが出現する確率
            - 日本語なら n = 2, 3くらい 
        - P(ダイスキ|ラーメン) = P(ラーメンダイスキ) / P(ラーメン)
- 冪乗則
- 表記ゆれ: 一番悩ましい前処理
- コーパスの単語を全部覚えていっても未知語数は0でない値に収束
- Distributional Hypothesis  
https://en.wikipedia.org/wiki/Distributional_semantics
    - words that occur in the same contexts tend to have similar meanings
    - 周辺語をみることで語の意味を推測できそう
- 言語の分散表現
    - 共起語ベクトル
        - 低次元のベクトルで表す→類似度の計算ができる
        - word2vec?
        - 例: 共起行列 係り受け関係の名詞，形容詞
            - 特異値分解
    - DL
        - word2vec (2013)
            - skipgram: ターゲット語から周辺語を予測→次元圧縮？
            - 単語-単語の関係（国-首都など）をベクトルで表現できる
                - 意味のたしざん・ひきざんができる
    


